{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71885,"databundleVersionId":8143495,"sourceType":"competition"},{"sourceId":7884485,"sourceType":"datasetVersion","datasetId":4628051},{"sourceId":7884725,"sourceType":"datasetVersion","datasetId":4628331},{"sourceId":4534,"sourceType":"modelInstanceVersion","modelInstanceId":3326,"modelId":986},{"sourceId":17191,"sourceType":"modelInstanceVersion","modelInstanceId":14317,"modelId":21716},{"sourceId":17555,"sourceType":"modelInstanceVersion","modelInstanceId":14611,"modelId":22086}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\"> üì∏ Image Matching Challenge - üìä Understanding the baseline</center>\n<p><center style=\"color:#949494; font-family: consolas; font-size: 20px;\">Reconstruct 3D scenes from 2D images over six different domains</center></p>\n\n***\n\nIn this notebook I explain the baseline solution provided by the organizers in [this notebook](https://www.kaggle.com/code/oldufo/imc-2024-submission-example).\n\nI have made the code a bit easier to read, adding comments and type annotations to make it easier for you to understand what is going on.\n\nHope you enjoy ‚ù§Ô∏è","metadata":{}},{"cell_type":"markdown","source":"# Structure from Motion","metadata":{}},{"cell_type":"markdown","source":"Structure from Motion (SfM) is the name given to the procedure of **reconstructing a 3D scene and simultaneously obtaining the camera poses of a camera w.r.t. the given scene**. This means that, as the name suggests, we are creating the entire rigid structure from a set of images with different view points (or equivalently a camera in motion).\n\nIn this competition, the important aspect of SfM we are interested in is *obtaining the camera poses* of where each image was taken, described by a rotation matrix and translation vector from the origin. These are the objects that will be scored in our submission!","metadata":{}},{"cell_type":"markdown","source":"<center><img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.ENP48SmZHwG3r3O0lUVcWAHaFf%26pid%3DApi&f=1&ipt=550bf79efa85e7af870dd2d0a16793af7f3f83a36c14a1f4a648659a384b5a98&ipo=images\" alt=\"Structure from motion structure: multiple cameras pointing toward an object in different positions and rotations that we need to find.\"></center> ","metadata":{}},{"cell_type":"markdown","source":"# Baseline solution steps\nIn order to be able to estimate the camera poses, the solution provided by the organizers consists in the following steps:\n\n* [1. Find pairs of images that are similar](#1)\n* [2. Compute image keypoints](#2)\n* [3. Match keypoints between images](#3)\n* [4. Outlier detection with RANSAC](#4)\n* [5. Sparse reconstruction](#5)\n\nLet's understand how these steps are carried out","metadata":{}},{"cell_type":"markdown","source":"# Installing & importing relevant packages and models","metadata":{}},{"cell_type":"code","source":"!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp /kaggle/input/aliked/pytorch/aliked-n16/1/* /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/* /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-24T21:09:46.942962Z","iopub.execute_input":"2024-04-24T21:09:46.943713Z","iopub.status.idle":"2024-04-24T21:09:54.354271Z","shell.execute_reply.started":"2024-04-24T21:09:46.943676Z","shell.execute_reply":"2024-04-24T21:09:54.352711Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\nkornia is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nkornia-moons is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nkornia-rs is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nlightglue is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\npycolmap is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nrerun-sdk is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}]},{"cell_type":"code","source":"# General utilities\nimport matplotlib.pyplot as plt\n\nimport os\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom time import time, sleep\nfrom fastprogress import progress_bar\nimport gc\nimport numpy as np\nimport h5py\nfrom IPython.display import clear_output\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom typing import Any\nimport itertools\nimport pandas as pd\n\n# CV/MLe\nimport cv2\nimport torch\nfrom torch import Tensor as T\nimport torch.nn.functional as F\nimport kornia as K\nimport kornia.feature as KF\nfrom PIL import Image\nfrom transformers import AutoImageProcessor, AutoModel\n\nimport torch\nfrom lightglue import match_pair\nfrom lightglue import LightGlue, ALIKED\nfrom lightglue.utils import load_image, rbd\n\n# 3D reconstruction\nimport pycolmap\n\n# Data importing into colmap\nimport sys\nsys.path.append(\"/kaggle/input/colmap-db-import\")\n\n# Provided by organizers\nfrom database import *\nfrom h5_to_db import *\n\ndef arr_to_str(a):\n    \"\"\"Returns ;-separated string representing the input\"\"\"\n    return \";\".join([str(x) for x in a.reshape(-1)])\n\ndef load_torch_image(file_name: Path | str, device=torch.device(\"cpu\")):\n    \"\"\"Loads an image and adds batch dimension\"\"\"\n    img = K.io.load_image(file_name, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n    return img\n\ndevice = K.utils.get_cuda_device_if_available(0)\nprint(device)\n\nDEBUG = len([p for p in Path(\"/kaggle/input/image-matching-challenge-2024/test/\").iterdir() if p.is_dir()]) == 2\nprint(\"DEBUG:\", DEBUG)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-24T21:09:54.357356Z","iopub.execute_input":"2024-04-24T21:09:54.357736Z","iopub.status.idle":"2024-04-24T21:09:54.377045Z","shell.execute_reply.started":"2024-04-24T21:09:54.357697Z","shell.execute_reply":"2024-04-24T21:09:54.376005Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"cuda:0\nDEBUG: False\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# Finding image pairs\n\nTo find pairs of similar images, we use [DINOv2](https://arxiv.org/pdf/2304.07193.pdf) to obtain normalized image embeddings.\n\n<center><img src=\"https://www.labellerr.com/blog/content/images/2023/05/Dino-v2-20230419.jpg\" alt=\"DINOv2 example\"></center> \nThen, we calculate the distances between all the embeddings, and only keep those below a given distance threshold. For images with less than a set minimum number of pairs, the closest ones are kept instead.","metadata":{}},{"cell_type":"code","source":"def embed_images(\n    paths: list[Path],\n    model_name: str,\n    device: torch.device = torch.device(\"cpu\"),\n) -> T:\n    \"\"\"Computes image embeddings.\n    \n    Returns a tensor of shape [len(filenames), output_dim]\n    \"\"\"\n    processor = AutoImageProcessor.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(model_name).eval().to(device)\n    \n    embeddings = []\n    \n    for i, path in tqdm(enumerate(paths), desc=\"Global descriptors\"):\n        image = load_torch_image(path)\n        \n        with torch.inference_mode():\n            inputs = processor(images=image, return_tensors=\"pt\", do_rescale=False).to(device)\n            outputs = model(**inputs) # last_hidden_state and pooled\n            \n            # Max pooling over all the hidden states but the first (starting token)\n            # To obtain a tensor of shape [1, output_dim]\n            # We normalize so that distances are computed in a better fashion later\n            embedding = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=-1, p=2)\n            \n        embeddings.append(embedding.detach().cpu())\n    return torch.cat(embeddings, dim=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.378371Z","iopub.execute_input":"2024-04-24T21:09:54.378683Z","iopub.status.idle":"2024-04-24T21:09:54.686251Z","shell.execute_reply.started":"2024-04-24T21:09:54.378657Z","shell.execute_reply":"2024-04-24T21:09:54.684981Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"def get_pairs_exhaustive(lst: list[Any]) -> list[tuple[int, int]]:\n    \"\"\"Obtains all possible index pairs of a list\"\"\"\n    return list(itertools.combinations(range(len(lst)), 2))            \n    \ndef get_image_pairs(\n    paths: list[Path],\n    model_name: str,\n    similarity_threshold: float = 0.5,#0.6,\n    tolerance: int = 1000,\n    min_matches: int = 25,#25\n    exhaustive_if_less: int = 20,\n    p: float = 2.0,\n    device: torch.device = torch.device(\"cpu\"),\n) -> list[tuple[int, int]]:\n    \"\"\"Obtains pairs of similar images\"\"\"\n    if len(paths) <= exhaustive_if_less:\n        return get_pairs_exhaustive(paths)\n    \n    matches = []\n    \n    # Embed images and compute distances for filtering\n    embeddings = embed_images(paths, model_name)\n    distances = torch.cdist(embeddings, embeddings, p=p)\n    \n    # Remove pairs above similarity threshold (if enough)\n    mask = distances <= similarity_threshold\n    image_indices = np.arange(len(paths))\n    \n    for current_image_index in range(len(paths)):\n        mask_row = mask[current_image_index]\n        indices_to_match = image_indices[mask_row]\n        \n        # We don't have enough matches below the threshold, we pick most similar ones\n        if len(indices_to_match) < min_matches:\n            indices_to_match = np.argsort(distances[current_image_index])[:min_matches]\n            \n        for other_image_index in indices_to_match:\n            # Skip an image matching itself\n            if other_image_index == current_image_index:\n                continue\n            \n            # We need to check if we are below a certain distance tolerance \n            # since for images that don't have enough matches, we picked\n            # the most similar ones (which could all still be very different \n            # to the image we are analyzing)\n            if distances[current_image_index, other_image_index] < tolerance:\n                # Add the pair in a sorted manner to avoid redundancy\n                matches.append(tuple(sorted((current_image_index, other_image_index.item()))))\n                \n    return sorted(list(set(matches)))","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.689643Z","iopub.execute_input":"2024-04-24T21:09:54.690103Z","iopub.status.idle":"2024-04-24T21:09:54.707129Z","shell.execute_reply.started":"2024-04-24T21:09:54.690058Z","shell.execute_reply":"2024-04-24T21:09:54.705834Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    images_list = list(Path(\"/kaggle/input/image-matching-challenge-2024/test/church/images/\").glob(\"*.png\"))[:10]\n    index_pairs = get_image_pairs(images_list, \"/kaggle/input/dinov2/pytorch/base/1\")\n    print(index_pairs)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.708465Z","iopub.execute_input":"2024-04-24T21:09:54.708798Z","iopub.status.idle":"2024-04-24T21:09:54.720976Z","shell.execute_reply.started":"2024-04-24T21:09:54.708770Z","shell.execute_reply":"2024-04-24T21:09:54.719874Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# Computing keypoints\n\nIn order to be able to know the position of each camera, we must be able to relate images to each other. For this, we extract relevant keypoints and compare pairs of image keypoints against each other. There are many ways to extract relevant keypoints, the most traditional one being [SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). However, newer and improved methods exist now, one of which is [ALIKED](https://arxiv.org/abs/2304.03608), the keypoint extraction method used in the solution.\n\n<center><img src=\"https://www.catalyzex.com/_next/image?url=https%3A%2F%2Fd3i71xaburhd42.cloudfront.net%2Faf9fc17471b4c38211c3d9f5058c9c1f59501eea%2F3-Figure1-1.png&w=640&q=75\" alt=\"ALIKED architecture\"></center> \n\n\nLet's take a closer look at the keypoints that ALIKED extracts.","metadata":{}},{"cell_type":"code","source":"if DEBUG:\n    dtype = torch.float32 # ALIKED has issues with float16\n\n    extractor = ALIKED(\n            max_num_keypoints=4096, \n            detection_threshold=0.005, #0.01\n            resize=1024\n        ).eval().to(device, dtype)\n\n    path = images_list[0]\n    image = load_torch_image(path, device=device).to(dtype)\n    features = extractor.extract(image)\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 20))\n    ax[0].imshow(image[0, ...].permute(1,2,0).cpu())\n    ax[1].imshow(image[0, ...].permute(1,2,0).cpu())\n    ax[1].scatter(features[\"keypoints\"][0, :, 0].cpu(), features[\"keypoints\"][0, :, 1].cpu(), s=0.5, c=\"red\")\n\n    del extractor","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.722349Z","iopub.execute_input":"2024-04-24T21:09:54.722650Z","iopub.status.idle":"2024-04-24T21:09:54.732759Z","shell.execute_reply.started":"2024-04-24T21:09:54.722623Z","shell.execute_reply":"2024-04-24T21:09:54.731694Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"def detect_keypoints(\n    paths: list[Path],\n    feature_dir: Path,\n    num_features: int = 4096,\n    resize_to: int = 1024,\n    device: torch.device = torch.device(\"cpu\"),\n) -> None:\n    \"\"\"Detects the keypoints in a list of images with ALIKED\n    \n    Stores them in feature_dir/keypoints.h5 and feature_dir/descriptors.h5\n    to be used later with LightGlue\n    \"\"\"\n    dtype = torch.float32 # ALIKED has issues with float16\n    \n    extractor = ALIKED(\n        max_num_keypoints=num_features, \n        detection_threshold=0.005, #0.01\n        resize=resize_to\n    ).eval().to(device, dtype)\n    \n    feature_dir.mkdir(parents=True, exist_ok=True)\n    \n    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"w\") as f_keypoints, \\\n         h5py.File(feature_dir / \"descriptors.h5\", mode=\"w\") as f_descriptors:\n        \n        for path in tqdm(paths, desc=\"Computing keypoints\"):\n            key = path.name\n            \n            with torch.inference_mode():\n                image = load_torch_image(path, device=device).to(dtype)\n                features = extractor.extract(image)\n                \n                f_keypoints[key] = features[\"keypoints\"].squeeze().detach().cpu().numpy()\n                f_descriptors[key] = features[\"descriptors\"].squeeze().detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.734166Z","iopub.execute_input":"2024-04-24T21:09:54.734518Z","iopub.status.idle":"2024-04-24T21:09:54.750019Z","shell.execute_reply.started":"2024-04-24T21:09:54.734488Z","shell.execute_reply":"2024-04-24T21:09:54.748914Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    feature_dir = Path(\"./sample_test_features\")\n    detect_keypoints(images_list, feature_dir)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.751982Z","iopub.execute_input":"2024-04-24T21:09:54.752454Z","iopub.status.idle":"2024-04-24T21:09:54.760459Z","shell.execute_reply.started":"2024-04-24T21:09:54.752411Z","shell.execute_reply":"2024-04-24T21:09:54.759109Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# Match and compute keypoint distances\n\nNow that we have the relevant image pairs and keypoints, we can go ahead and compare the keypoints of the images in a pair to find a good relationship between them. This is done with [LightGlue](https://arxiv.org/abs/2306.13643), which matches the keypoints and their descriptors between two images.\n\n<center><img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fraw.githubusercontent.com%2Fcvg%2Flightglue%2Fmaster%2Fassets%2Feasy_hard.jpg&f=1&nofb=1&ipt=60962b56b05d3e8f95a064ab2a6010e5a6cbd5f1d10379d90e660b2561a3bae9&ipo=images\" alt=\"LightGlue example\"></center> ","metadata":{}},{"cell_type":"code","source":"if DEBUG:\n    matcher_params = {\n        \"width_confidence\": -1,\n        \"depth_confidence\": -1,\n        \"mp\": True if 'cuda' in str(device) else False,\n    }\n    matcher = KF.LightGlueMatcher(\"aliked\", matcher_params).eval().to(device)\n\n    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"r\") as f_keypoints, \\\n         h5py.File(feature_dir / \"descriptors.h5\", mode=\"r\") as f_descriptors:\n            idx1, idx2 = index_pairs[0]\n            key1, key2 = images_list[idx1].name, images_list[idx2].name\n\n            keypoints1 = torch.from_numpy(f_keypoints[key1][...]).to(device)\n            keypoints2 = torch.from_numpy(f_keypoints[key2][...]).to(device)\n            print(\"Keypoints:\", keypoints1.shape, keypoints2.shape)\n            descriptors1 = torch.from_numpy(f_descriptors[key1][...]).to(device)\n            descriptors2 = torch.from_numpy(f_descriptors[key2][...]).to(device)\n            print(\"Descriptors:\", descriptors1.shape, descriptors2.shape)\n\n            with torch.inference_mode():\n                distances, indices = matcher(\n                    descriptors1, \n                    descriptors2, \n                    KF.laf_from_center_scale_ori(keypoints1[None]),\n                    KF.laf_from_center_scale_ori(keypoints2[None]),\n                )\n    print(distances, indices)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.761992Z","iopub.execute_input":"2024-04-24T21:09:54.762424Z","iopub.status.idle":"2024-04-24T21:09:54.777574Z","shell.execute_reply.started":"2024-04-24T21:09:54.762384Z","shell.execute_reply":"2024-04-24T21:09:54.776276Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def keypoint_distances(\n    paths: list[Path],\n    index_pairs: list[tuple[int, int]],\n    feature_dir: Path,\n    min_matches: int = 15,\n    verbose: bool = True,\n    device: torch.device = torch.device(\"cpu\"),\n) -> None:\n    \"\"\"Computes distances between keypoints of images.\n    \n    Stores output at feature_dir/matches.h5\n    \"\"\"\n    \n    matcher_params = {\n        \"width_confidence\": -1,\n        \"depth_confidence\": -1,\n        \"mp\": True if 'cuda' in str(device) else False,\n    }\n    matcher = KF.LightGlueMatcher(\"aliked\", matcher_params).eval().to(device)\n    \n    with h5py.File(feature_dir / \"keypoints.h5\", mode=\"r\") as f_keypoints, \\\n         h5py.File(feature_dir / \"descriptors.h5\", mode=\"r\") as f_descriptors, \\\n         h5py.File(feature_dir / \"matches.h5\", mode=\"w\") as f_matches:\n        \n            for idx1, idx2 in tqdm(index_pairs, desc=\"Computing keypoing distances\"):\n                key1, key2 = paths[idx1].name, paths[idx2].name\n\n                keypoints1 = torch.from_numpy(f_keypoints[key1][...]).to(device)\n                keypoints2 = torch.from_numpy(f_keypoints[key2][...]).to(device)\n                descriptors1 = torch.from_numpy(f_descriptors[key1][...]).to(device)\n                descriptors2 = torch.from_numpy(f_descriptors[key2][...]).to(device)\n\n                with torch.inference_mode():\n                    distances, indices = matcher(\n                        descriptors1, \n                        descriptors2, \n                        KF.laf_from_center_scale_ori(keypoints1[None]),\n                        KF.laf_from_center_scale_ori(keypoints2[None]),\n                    )\n\n                # We have matches to consider\n                n_matches = len(indices)\n                if n_matches:\n                    if verbose:\n                        print(f\"{key1}-{key2}: {n_matches} matches\")\n                    # Store the matches in the group of one image\n                    if n_matches >= min_matches:\n                        group  = f_matches.require_group(key1)\n                        group.create_dataset(key2, data=indices.detach().cpu().numpy().reshape(-1, 2))","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.782841Z","iopub.execute_input":"2024-04-24T21:09:54.783185Z","iopub.status.idle":"2024-04-24T21:09:54.801337Z","shell.execute_reply.started":"2024-04-24T21:09:54.783155Z","shell.execute_reply":"2024-04-24T21:09:54.799962Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    keypoint_distances(images_list, index_pairs, feature_dir, verbose=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.802770Z","iopub.execute_input":"2024-04-24T21:09:54.803174Z","iopub.status.idle":"2024-04-24T21:09:54.814207Z","shell.execute_reply.started":"2024-04-24T21:09:54.803135Z","shell.execute_reply":"2024-04-24T21:09:54.813075Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# RANSAC\nUp to now, we have matched keypoints and their descriptors extracted from pairs of images. This is described by a [fundamental matrix](https://en.wikipedia.org/wiki/Fundamental_matrix_(computer_vision)) denoted as $F$. In epipolar geometry, with homogeneous image coordinates, $x$ and $x‚Ä≤$, of corresponding points in a stereo image pair, $Fx$ describes a line (an epipolar line) on which the corresponding point $x‚Ä≤$ on the other image must lie. That means, for all pairs of corresponding points, $x'Fx = 0$ holds. This is known as epipolar constraint or correspondance condition (or Longuet-Higgins equation), and is solved via the [eight-point algorithm](https://en.wikipedia.org/wiki/Eight-point_algorithm).\n\n<center><img src=\"https://cmsc426.github.io/assets/sfm/epipole1.png\" alt=\"Fundamental matrix\"></center>\n\nSince the keypoint correspondences are computed using feature descriptors, the data is bound to be noisy and (in general) contains several outliers. Thus, to remove these outliers, we use a [RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus) algorithm to find the best possible fundamental matrix. So, out of all possibilities, the $F$ matrix with maximum number of inliers is chosen.\n\n<center><img src=\"https://cmsc426.github.io/assets/sfm/ransac.png\" alt=\"RANSAC\"></center>","metadata":{}},{"cell_type":"code","source":"def import_into_colmap(\n    path: Path,\n    feature_dir: Path,\n    database_path: str = \"colmap.db\",\n) -> None:\n    \"\"\"Adds keypoints into colmap\"\"\"\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    fname_to_id = add_keypoints(db, feature_dir, path, \"\", \"simple-pinhole\", single_camera)\n    add_matches(\n        db,\n        feature_dir,\n        fname_to_id,\n    )\n    db.commit()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.815854Z","iopub.execute_input":"2024-04-24T21:09:54.816974Z","iopub.status.idle":"2024-04-24T21:09:54.824428Z","shell.execute_reply.started":"2024-04-24T21:09:54.816935Z","shell.execute_reply":"2024-04-24T21:09:54.823357Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    database_path = \"colmap.db\"\n    images_dir = images_list[0].parent\n    import_into_colmap(\n        images_dir, \n        feature_dir, \n        database_path,\n    )\n\n    # This does RANSAC\n    pycolmap.match_exhaustive(database_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.825835Z","iopub.execute_input":"2024-04-24T21:09:54.826178Z","iopub.status.idle":"2024-04-24T21:09:54.839458Z","shell.execute_reply.started":"2024-04-24T21:09:54.826149Z","shell.execute_reply":"2024-04-24T21:09:54.838251Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# Sparse Reconstruction\n\nNow we have similar image pairs, with matched keypoint descriptors, without outliers! All that is left is to construct the scene and obtain the camera positions. We do this with pycolmap, which offers an incremental reconstruction algorithm that starts from two pairs of images and continually adds more and more images to the scene, resulting in a reconstructed scene with camera information. We can then use the camera rotation and translation as our submission!","metadata":{}},{"cell_type":"code","source":"if DEBUG:\n    mapper_options = pycolmap.IncrementalPipelineOptions()\n    mapper_options.min_model_size = 3\n    mapper_options.max_num_models = 2\n\n    maps = pycolmap.incremental_mapping(\n        database_path=database_path, \n        image_path=images_dir,\n        output_path=Path.cwd() / \"incremental_pipeline_outputs\", \n        options=mapper_options,\n    )","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-24T21:09:54.841063Z","iopub.execute_input":"2024-04-24T21:09:54.841530Z","iopub.status.idle":"2024-04-24T21:09:54.850744Z","shell.execute_reply.started":"2024-04-24T21:09:54.841500Z","shell.execute_reply":"2024-04-24T21:09:54.849415Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"if DEBUG:\n    print(maps[0].summary())\n    for k, im in maps[0].images.items():\n        print(\"Rotation\", im.cam_from_world.rotation.matrix(), \"Translation:\", im.cam_from_world.translation, sep=\"\\n\")\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.852217Z","iopub.execute_input":"2024-04-24T21:09:54.852594Z","iopub.status.idle":"2024-04-24T21:09:54.860477Z","shell.execute_reply.started":"2024-04-24T21:09:54.852565Z","shell.execute_reply":"2024-04-24T21:09:54.859383Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"# Running everything","metadata":{}},{"cell_type":"code","source":"def parse_sample_submission(\n    base_path: Path,\n) -> dict[dict[str, list[Path]]]:\n    \"\"\"Construct a dict describing the test data as \n    \n    {\"dataset\": {\"scene\": [<image paths>]}}\n    \"\"\"\n    data_dict = {}\n    with open(base_path / \"sample_submission.csv\", \"r\") as f:\n        for i, l in enumerate(f):\n            # Skip header\n            if i == 0:\n                print(\"header:\", l)\n\n            if l and i > 0:\n                image_path, dataset, scene, _, _ = l.strip().split(',')\n                if dataset not in data_dict:\n                    data_dict[dataset] = {}\n                if scene not in data_dict[dataset]:\n                    data_dict[dataset][scene] = []\n                data_dict[dataset][scene].append(Path(base_path / image_path))\n\n    for dataset in data_dict:\n        for scene in data_dict[dataset]:\n            print(f\"{dataset} / {scene} -> {len(data_dict[dataset][scene])} images\")\n\n    return data_dict","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-24T21:09:54.861944Z","iopub.execute_input":"2024-04-24T21:09:54.862259Z","iopub.status.idle":"2024-04-24T21:09:54.872164Z","shell.execute_reply.started":"2024-04-24T21:09:54.862232Z","shell.execute_reply":"2024-04-24T21:09:54.871086Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"def create_submission(\n    results: dict,\n    data_dict: dict[dict[str, list[Path]]],\n    base_path: Path,\n) -> None:\n    \"\"\"Prepares a submission file.\"\"\"\n    \n    with open(\"submission.csv\", \"w\") as f:\n        f.write(\"image_path,dataset,scene,rotation_matrix,translation_vector\\n\")\n        \n        for dataset in data_dict:\n            # Only write results for datasets with images that have results \n            if dataset in results:\n                res = results[dataset]\n            else:\n                res = {}\n            \n            # Same for scenes\n            for scene in data_dict[dataset]:\n                if scene in res:\n                    scene_res = res[scene]\n                else:\n                    scene_res = {\"R\":{}, \"t\":{}}\n                    \n                # Write the row with rotation and translation matrices\n                for image in data_dict[dataset][scene]:\n                    if image in scene_res:\n                        print(image)\n                        R = scene_res[image][\"R\"].reshape(-1)\n                        T = scene_res[image][\"t\"].reshape(-1)\n                    else:\n                        R = np.eye(3).reshape(-1)\n                        T = np.zeros((3))\n                    image_path = str(image.relative_to(base_path))\n                    f.write(f\"{image_path},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-24T21:09:54.873451Z","iopub.execute_input":"2024-04-24T21:09:54.873760Z","iopub.status.idle":"2024-04-24T21:09:54.888236Z","shell.execute_reply.started":"2024-04-24T21:09:54.873733Z","shell.execute_reply":"2024-04-24T21:09:54.886949Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"class Config:\n    base_path: Path = Path(\"/kaggle/input/image-matching-challenge-2024\")\n    feature_dir: Path = Path.cwd() / \".feature_outputs\"\n        \n    device: torch.device = K.utils.get_cuda_device_if_available(0)\n    \n    pair_matching_args = {\n        \"model_name\": \"/kaggle/input/dinov2/pytorch/base/1\",\n        \"similarity_threshold\": 0.3,\n        \"tolerance\": 500,\n        \"min_matches\": 50,\n        \"exhaustive_if_less\": 50,\n        \"p\": 2.0,\n    }\n    \n    keypoint_detection_args = {\n        \"num_features\": 4096,\n        \"resize_to\": 1024,\n    }\n    \n    keypoint_distances_args = {\n        \"min_matches\": 15,\n        \"verbose\": False,\n    }\n    \n    colmap_mapper_options = {\n        \"min_model_size\": 3, # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n        \"max_num_models\": 2,\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.889573Z","iopub.execute_input":"2024-04-24T21:09:54.889892Z","iopub.status.idle":"2024-04-24T21:09:54.903189Z","shell.execute_reply.started":"2024-04-24T21:09:54.889865Z","shell.execute_reply":"2024-04-24T21:09:54.901858Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def run_from_config(config: Config) -> None:\n    results = {}\n    \n    data_dict = parse_sample_submission(config.base_path)\n    datasets = list(data_dict.keys())\n    \n    for dataset in datasets:\n        if dataset not in results:\n            results[dataset] = {}\n            \n        for scene in data_dict[dataset]:\n            images_dir = data_dict[dataset][scene][0].parent\n            results[dataset][scene] = {}\n            image_paths = data_dict[dataset][scene]\n            print (f\"Got {len(image_paths)} images\")\n            \n            try:\n                feature_dir = config.feature_dir / f\"{dataset}_{scene}\"\n                feature_dir.mkdir(parents=True, exist_ok=True)\n                database_path = feature_dir / \"colmap.db\"\n                if database_path.exists():\n                    database_path.unlink()\n                \n                # 1. Get the pairs of images that are somewhat similar\n                index_pairs = get_image_pairs(\n                    image_paths,\n                    **config.pair_matching_args,\n                    device=config.device,\n                )\n                gc.collect()\n                \n                # 2. Detect keypoints of all images\n                detect_keypoints(\n                    image_paths,\n                    feature_dir,\n                    **config.keypoint_detection_args,\n                    device=device,\n                )\n                gc.collect()\n                \n                # 3. Match  keypoints of pairs of similar images\n                keypoint_distances(\n                    image_paths, \n                    index_pairs, \n                    feature_dir,\n                    **config.keypoint_distances_args,\n                    device=device,\n                )\n                gc.collect()\n                \n                sleep(1)\n                \n                # 4.1. Import keypoint distances of matches into colmap for RANSAC \n                import_into_colmap(\n                    images_dir, \n                    feature_dir, \n                    database_path,\n                )\n                \n                output_path = feature_dir / \"colmap_rec_aliked\"\n                output_path.mkdir(parents=True, exist_ok=True)\n                \n                # 4.2. Compute RANSAC (detect match outliers)\n                # By doing it exhaustively we guarantee we will find the best possible configuration\n                pycolmap.match_exhaustive(database_path)\n                \n                mapper_options = pycolmap.IncrementalPipelineOptions(**config.colmap_mapper_options)\n                \n                # 5.1 Incrementally start reconstructing the scene (sparse reconstruction)\n                # The process starts from a random pair of images and is incrementally extended by \n                # registering new images and triangulating new points.\n                maps = pycolmap.incremental_mapping(\n                    database_path=database_path, \n                    image_path=images_dir,\n                    output_path=output_path, \n                    options=mapper_options,\n                )\n                \n                print(maps)\n                clear_output(wait=False)\n                \n                # 5.2. Look for the best reconstruction: The incremental mapping offered by \n                # pycolmap attempts to reconstruct multiple models, we must pick the best one\n                images_registered  = 0\n                best_idx = None\n                \n                print (\"Looking for the best reconstruction\")\n            \n                if isinstance(maps, dict):\n                    for idx1, rec in maps.items():\n                        print(idx1, rec.summary())\n                        try:\n                            if len(rec.images) > images_registered:\n                                images_registered = len(rec.images)\n                                best_idx = idx1\n                        except Exception:\n                            continue\n                \n                # Parse the reconstruction object to get the rotation matrix and translation vector\n                # obtained for each image in the reconstruction\n                if best_idx is not None:\n                    for k, im in maps[best_idx].images.items():\n                        key = config.base_path / \"test\" / scene / \"images\" / im.name\n                        results[dataset][scene][key] = {}\n                        results[dataset][scene][key][\"R\"] = deepcopy(im.cam_from_world.rotation.matrix())\n                        results[dataset][scene][key][\"t\"] = deepcopy(np.array(im.cam_from_world.translation))\n                        \n                print(f\"Registered: {dataset} / {scene} -> {len(results[dataset][scene])} images\")\n                print(f\"Total: {dataset} / {scene} -> {len(data_dict[dataset][scene])} images\")\n                create_submission(results, data_dict, config.base_path)\n                gc.collect()\n            \n            except Exception as e:\n                print(e)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.904608Z","iopub.execute_input":"2024-04-24T21:09:54.904990Z","iopub.status.idle":"2024-04-24T21:09:54.933426Z","shell.execute_reply.started":"2024-04-24T21:09:54.904961Z","shell.execute_reply":"2024-04-24T21:09:54.932228Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"run_from_config(Config)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:09:54.934982Z","iopub.execute_input":"2024-04-24T21:09:54.935474Z","iopub.status.idle":"2024-04-24T21:15:01.722866Z","shell.execute_reply.started":"2024-04-24T21:09:54.935433Z","shell.execute_reply":"2024-04-24T21:15:01.721735Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"Looking for the best reconstruction\n0 Reconstruction:\n\tnum_reg_images = 40\n\tnum_cameras = 40\n\tnum_points3D = 12515\n\tnum_observations = 64619\n\tmean_track_length = 5.16332\n\tmean_observations_per_image = 1615.47\n\tmean_reprojection_error = 0.914313\nRegistered: church / church -> 40 images\nTotal: church / church -> 41 images\n/kaggle/input/image-matching-challenge-2024/test/church/images/00046.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00090.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00092.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00087.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00050.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00068.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00083.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00096.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00069.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00081.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00042.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00018.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00030.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00024.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00032.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00026.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00037.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00008.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00035.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00021.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00010.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00039.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00011.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00013.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00006.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00012.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00029.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00001.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00072.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00066.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00104.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00058.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00059.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00111.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00061.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00060.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00074.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00102.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00076.png\n/kaggle/input/image-matching-challenge-2024/test/church/images/00063.png\n","output_type":"stream"}]},{"cell_type":"code","source":"!cat submission.csv","metadata":{"execution":{"iopub.status.busy":"2024-04-24T21:15:01.724456Z","iopub.execute_input":"2024-04-24T21:15:01.724781Z","iopub.status.idle":"2024-04-24T21:15:02.875397Z","shell.execute_reply.started":"2024-04-24T21:15:01.724753Z","shell.execute_reply":"2024-04-24T21:15:02.874371Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"image_path,dataset,scene,rotation_matrix,translation_vector\ntest/church/images/00046.png,church,church,-0.2200748863788864;-0.3383640516191441;-0.9149190198903934;0.3641637781198774;0.8416086883446777;-0.3988477885211449;0.9049595499749024;-0.4209567486982817;-0.061997005045361986,5.811871745067959;5.073738291372837;10.83452336666038\ntest/church/images/00090.png,church,church,0.9984892955948547;0.0016773560947713978;-0.05492097103131295;-0.011673952133316231;0.9831903152339443;-0.18221010639304927;0.05369213558874357;0.18257598556959406;0.981724586668385,0.016281244087706443;0.011601577352781917;-0.7125567088927723\ntest/church/images/00092.png,church,church,0.9371735528644375;-0.1214719597581227;-0.3270325592413826;0.1429896979607324;0.9888119967406295;0.04248271859130434;0.31821325881953116;-0.08657596718680341;0.9440576909369065,0.18497414381818955;-0.10089140717848372;-0.4884140642203461\ntest/church/images/00087.png,church,church,0.8791033673008676;-0.16142370641522627;-0.4484636625273521;0.1816592417207624;0.9833592119968545;0.002140111764121884;0.44065540901901284;-0.08334894833227441;0.8937985026359075,0.9007209966864629;-0.04641816445774152;-0.4919806952198166\ntest/church/images/00050.png,church,church,0.9395775236728502;0.173582880919092;0.29506450220354014;-0.19568371192597997;0.9795467249195398;0.0468625499335404;-0.28089493034881335;-0.1017703156683314;0.9543274285868051,-2.8574531777815;0.37346985408009603;0.7005087830581649\ntest/church/images/00068.png,church,church,0.5555008893157563;-0.2193265763909513;-0.8020689589168929;0.5293188539189485;0.8371787614465134;0.13767088388186033;0.6412802139902142;-0.5010265205271135;0.5811472385490266,4.943649041060852;0.41936111999587417;0.41911996171730515\ntest/church/images/00083.png,church,church,0.9265399261010617;-0.14406219262680098;-0.3475195677889196;0.17661842570250397;0.9822146736676327;0.06372022076511388;0.3321591441508243;-0.12041768760254554;0.9355051488203834,1.5297566389726234;-0.09350993863018608;-0.6399010291586172\ntest/church/images/00096.png,church,church,0.9501193518845996;0.10049137671025424;0.2952536204371866;-0.026565084861939176;0.969304753979472;-0.2444229739183146;-0.31075313906808155;0.22438756008226682;0.9236247665798383,-1.0536181964119085;0.005516094350758463;-1.1538170967082082\ntest/church/images/00069.png,church,church,0.5675676508592764;-0.2169792637410353;-0.7942209773132921;0.3386720542562742;0.940784942152369;-0.014997742670118122;0.7504453353602233;-0.26046821634464745;0.6074439125633906,4.971890544053577;0.25699716790930804;0.7051620532424595\ntest/church/images/00081.png,church,church,0.8790790498786771;-0.1282442173263115;-0.459100691337711;0.1517360746316432;0.9883151514503219;0.014468070676760036;0.45188072289021447;-0.08238071458836166;0.8882664184489517,1.5859307301496024;0.008650118792809268;-0.2806363042055869\ntest/church/images/00042.png,church,church,0.7599366475452418;0.20106145321844177;0.6181185838877419;-0.26356985226760443;0.9645855150299891;0.01028188552571252;-0.5941609417433396;-0.17073100545600686;0.7860176200840849,-4.945967701048136;2.5021435961275023;6.168558893557193\ntest/church/images/00018.png,church,church,0.9809292043761296;-0.05793947401807907;-0.18552873996370928;0.08599174543583292;0.9854032892151403;0.14692099006228287;0.17430810571775546;-0.16007303006593854;0.9715931809798775,0.24443840219468532;1.1197407799850405;2.5028483837773656\ntest/church/images/00030.png,church,church,0.9575621616612201;0.07819189865666089;0.2774179762365045;-0.14981315652609806;0.9572893374987254;0.24729161417835893;-0.24623306984720422;-0.2783579553303946;0.9283782225030399,-2.840183122440585;0.1245608218224715;0.4379302908113924\ntest/church/images/00024.png,church,church,0.8157715485593804;0.21483320535933953;0.5369948551299746;-0.23251855775977603;0.9719398014320176;-0.03561099113481375;-0.5295770962365576;-0.09581083585546013;0.8428335439897572,-0.10718353945903993;0.5071232112593278;1.8232557618623126\ntest/church/images/00032.png,church,church,0.9801772841058486;0.0850010711909593;0.17896175462727212;-0.10404635542666572;0.9895463895876421;0.09986139782964375;-0.1686026323795404;-0.11650219204140279;0.978774944307545,-1.0763693828309842;0.4806242440732087;1.374488853645158\ntest/church/images/00026.png,church,church,0.9721038654293653;0.1411291980533479;0.18734093058940796;-0.17870133697788354;0.9629791445011894;0.20183408884049872;-0.151920725971157;-0.2296817727042708;0.9613357250762193,-2.156318395557584;0.509972181700613;0.7076658215717821\ntest/church/images/00037.png,church,church,-0.770027259822857;0.20612734331605917;0.6037959402539622;-0.08899707135061077;0.9024173764726424;-0.4215713461933194;-0.6317733299378164;-0.37835749890506565;-0.6765412497413492,-0.639773621049655;0.4106994877076307;3.572654183511341\ntest/church/images/00008.png,church,church,0.9997230440379002;0.0001851647533872302;-0.023532975451663406;-6.491141563245607e-05;0.9999869384154308;0.00511065407178484;0.02353361438671523;-0.005107711086918819;0.9997099980901211,-0.8124453346980932;1.3228981266811728;4.50796261096413\ntest/church/images/00035.png,church,church,-0.4369830598056943;0.2231671444196225;0.8713450700465599;-0.3564354970485052;0.8464607419177188;-0.3955476568454046;-0.8258326355180194;-0.48342593853008786;-0.2903443129624992,-0.7316457043097729;1.18298045941942;3.443647156164905\ntest/church/images/00021.png,church,church,0.9773067140136482;0.1371540317065533;0.1614322097057556;-0.14927300967965515;0.9866275936950535;0.06544891092054989;-0.15029689060330842;-0.08806113186954997;0.9847111666518416,-1.7991058667005828;0.7088985611930003;1.7044294603349708\ntest/church/images/00010.png,church,church,0.9946267802008611;-0.017776162349611137;-0.10198811773627404;0.010860919280766723;0.9976287038942433;-0.0679632959667854;0.10295440029277282;0.06649042952495092;0.9924612910546905,-0.5591058240349603;0.9143571833543622;4.2997354890056725\ntest/church/images/00039.png,church,church,0.612749000142266;0.25324587149304445;0.748602158289957;-0.34229068310633914;0.9388482225508107;-0.037431314053598896;-0.7123031314568603;-0.23330354387559532;0.6619620120005671,-1.9594016605921882;0.7321380930624082;1.9148396399667744\ntest/church/images/00011.png,church,church,0.9998493391779791;0.012845626876802586;0.011674280084886753;-0.013007958164707576;0.9998182580475841;0.013937140993757199;-0.011493127065495878;-0.014086899759585745;0.999834719983968,0.26102234308646943;1.087839068648962;3.771176896723932\ntest/church/images/00013.png,church,church,0.9954122442658582;0.021137148743327956;0.09331497687193585;-0.010047910626377929;0.9929932064489676;-0.11774349849669999;-0.0951498999350594;0.11626569953860211;0.9886499803535869,0.5835788811622644;0.5795640105510603;3.9147189608664275\ntest/church/images/00006.png,church,church,0.9744065145128383;-0.07229040040764789;0.2128521610974239;0.08686462039239856;0.994416308687853;-0.059922806507282295;-0.20733181665877104;0.07687849520201756;0.9752452075126784,0.16121392754126548;0.9859527431275377;4.788727704070093\ntest/church/images/00012.png,church,church,0.9979949792589728;0.007426786232936085;-0.0628559004400743;-0.011246982410896028;0.9980961392102103;-0.060643237713049025;0.06228584719285158;0.06122858597030187;0.9961784646836861,0.18493757965654647;0.4822843150417238;2.5498162838947955\ntest/church/images/00029.png,church,church,0.9994906289566251;-0.005142155385184756;0.03149668023593324;0.004759664240003462;0.9999141659251674;0.012206816886453177;-0.03155674609671353;-0.012050685464801644;0.999429313536288,-0.9594361175671218;0.37478557322689854;1.6968536249283575\ntest/church/images/00001.png,church,church,0.9473850429914548;0.1047081926495618;0.3024859909287398;-0.12565897274502827;0.9907824846949457;0.05059536135234183;-0.29440007283413444;-0.08594336747982392;0.9518100307842339,1.190081707521812;1.5705039378119727;4.48771230551445\ntest/church/images/00098.png,church,church,1.0;0.0;0.0;0.0;1.0;0.0;0.0;0.0;1.0,0.0;0.0;0.0\ntest/church/images/00072.png,church,church,0.7345156478497332;-0.18449375220077818;-0.6530304881571491;0.3257677929864416;0.9400575081831887;0.10083266514952982;0.5952832167268196;-0.28679947124653515;0.7505890721134156,3.492519636122214;0.38908869657274886;0.19446994785358934\ntest/church/images/00066.png,church,church,0.5228353320729524;-0.24821629178483756;-0.8154948730855087;0.3385836923265915;0.9384350993572034;-0.06856126883983267;0.782307036160776;-0.24026701144147977;0.5746890153694625,4.935971086041709;0.4460972653100149;0.9163770487353526\ntest/church/images/00104.png,church,church,0.8006377942187528;0.151970305391791;0.579555130033076;-0.04088453810238949;0.9789005190728086;-0.2002054652179608;-0.5977521033109664;0.13659721826637478;0.7899579880912682,-0.29380611363840087;0.38846736004655646;0.5876190864412721\ntest/church/images/00058.png,church,church,0.9285157845623003;0.2965612134078335;0.223405202537294;-0.007103286718267654;0.6157734367112923;-0.7878912475453465;-0.3712249737510315;0.7299825486563619;0.5738619150289627,0.3812074898628359;0.0712541110972931;0.11087854229005133\ntest/church/images/00059.png,church,church,-0.17601667979608449;-0.2851158749591421;-0.942192690631723;0.5559330718020083;0.7610963685779829;-0.33417171845353755;0.8123770972340038;-0.5826158730962366;0.024539688383599545,6.199710496228257;2.8123213982096567;4.306844505483278\ntest/church/images/00111.png,church,church,0.8819132495467127;0.18823811596649595;0.4321983710882558;-0.26979238248447035;0.9533665548184932;0.13529332025961333;-0.3865761123340537;-0.23592079995392745;0.8915718061501205,-3.6884957808955567;0.9002748856347025;1.2324331332107805\ntest/church/images/00061.png,church,church,0.09923455261448832;0.30447142367397745;0.947338195014507;-0.4609060975705645;0.8578148747405746;-0.22741857860800552;-0.8818832534920231;-0.41406616963929044;0.22545761102826112,-3.498875474452669;1.7882126493522301;3.86044147179273\ntest/church/images/00060.png,church,church,0.9671964449624098;-0.08459283372588469;-0.23953097781769497;0.061043667199212226;0.9926924791465734;-0.10409280734374346;0.24658610574594034;0.08605634391522043;0.9652924935608881,0.4071198933362253;0.4771510111990148;0.7131061051750823\ntest/church/images/00074.png,church,church,0.8486600083511674;-0.16748125496058247;-0.5017232498721031;0.14299691454487068;0.9858726384438627;-0.08721825037446945;0.5092426461482222;0.002273764404035511;0.8606199842784221,3.553659863762284;0.19451629935249964;0.2798645546889265\ntest/church/images/00102.png,church,church,0.9228716543785042;-0.18387209030910218;-0.3383769554063104;0.11548998574077719;0.970355122157671;-0.2123040275076442;0.36738259721750277;0.1568502193423167;0.9167486220082304,-0.5649222099558173;0.5643533974660502;3.2239604019121693\ntest/church/images/00076.png,church,church,0.44261163574089357;-0.2573070271560668;-0.8590040941013483;0.2951478182048085;0.9463730637542556;-0.13139935163205763;0.8467483128508212;-0.19537430224185298;0.49481933744327056,4.711223814920408;1.2645366586057278;3.182648631448183\ntest/church/images/00063.png,church,church,0.6596117314062673;0.026759159245329484;-0.7511300228240844;0.42534527645693043;0.8106511300373234;0.4024004736142693;0.6196723001611447;-0.5849176803326208;0.5233331134634063,0.40599058896000567;0.5032978986370493;-0.09870869078434978\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# What to do next?\n\nHere are some ways in which you can explore potential improvements:\n\n- Using a different image embedding model to obtain the image pairs\n- Trying other approaches for keypoint extraction, such as SIFT or DISK\n- Leveraging the training data to train a better models for each dataset","metadata":{}}]}